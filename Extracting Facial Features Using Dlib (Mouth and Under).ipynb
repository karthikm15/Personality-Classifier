{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Extracting Facial Features Using Dlib (Mouth and Under).ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPa581AKcCpg7iRaVm6n+sj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"a2bNVSK2fG-7","executionInfo":{"status":"ok","timestamp":1614265986517,"user_tz":480,"elapsed":821,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}}},"source":["import collections\n","from collections import OrderedDict\n","FACIAL_LANDMARKS_IDXS = OrderedDict([\n","    (\"mouth\", (48, 68)),\n","    ('inner_mouth', (60, 68)),\n","    (\"right_eyebrow\", (17, 22)),\n","    (\"left_eyebrow\", (22, 27)),\n","    (\"right_eye\", (36, 42)),\n","    (\"left_eye\", (42, 48)),\n","    (\"nose\", (27, 35)),\n","    (\"jaw\", (0, 17))\n","])"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"51sSZyIEjuEs","executionInfo":{"status":"ok","timestamp":1614265988159,"user_tz":480,"elapsed":2438,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}}},"source":["from imutils import face_utils\n","import numpy as np\n","import argparse\n","import imutils\n","import dlib\n","import cv2\n","from google.colab.patches import cv2_imshow\n","# construct the argument parser and parse the arguments\n","ap = argparse.ArgumentParser()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lTnvuAQhl70","executionInfo":{"status":"ok","timestamp":1614265988161,"user_tz":480,"elapsed":2433,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}}},"source":["from google.colab import drive"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vIic8NishoTu","executionInfo":{"status":"ok","timestamp":1614266026926,"user_tz":480,"elapsed":41189,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}},"outputId":"6daf5f8d-a750-49ac-e56d-03709781f003"},"source":["drive.mount('/content/gdrive')"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1pkUnhaDhZZ1","executionInfo":{"status":"ok","timestamp":1614061991998,"user_tz":480,"elapsed":322,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}},"outputId":"bdd7c30f-6165-430c-98e3-d417fa57cf35"},"source":["! ls '/content/gdrive/MyDrive/personality_datasets/'"],"execution_count":52,"outputs":[{"output_type":"stream","text":[" celeba_dataset\t\t      refined_images\n","'Emotion Classifier'\t      shape_predictor_68_face_landmarks.dat\n"," headshot.jpg\t\t      test.csv\n"," images.jpeg\t\t      test_doublechin\n"," img_align_celeba\t      test_images\n"," img_celeba\t\t      train_doublechin\n"," lookup.csv\t\t      train_doublechin2\n"," my_model\t\t      train_images\n"," notebooks\t\t      valid_doublechin\n"," personality_images\t      valid_doublechin2\n","'Personality Workflow.gdoc'   valid_images\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mz-WE4_vfQ-n","executionInfo":{"status":"ok","timestamp":1614266030946,"user_tz":480,"elapsed":14894,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}}},"source":["detector = dlib.get_frontal_face_detector()\n","# Loads facial landmark predictor using path supplied\n","predictor = dlib.shape_predictor('/content/gdrive/MyDrive/personality_datasets/shape_predictor_68_face_landmarks.dat')"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"--XVnGYCgpfh","executionInfo":{"status":"ok","timestamp":1614266144278,"user_tz":480,"elapsed":772,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}},"outputId":"ed846f4d-6097-4d76-872b-6611f7d73cf3"},"source":["image = cv2.imread('/content/gdrive/MyDrive/personality_datasets/headshot.jpg') # takes from image headshot\n","# print(image)\n","image = imutils.resize(image, width=600, height=600) # rescales to width of 500 px\n","gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # converts to grayscale\n","# detect faces in the grayscale image\n","# Two arguments: grayscale image, number of image pyramid layers to apply when upscaling image prior to applying the detection\n","rects = detector(gray, 1) # creates the bounding box\n","len(rects)"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"8yYMp2Zsise1","executionInfo":{"status":"ok","timestamp":1614062159313,"user_tz":480,"elapsed":586,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}}},"source":["\n","# for (i, rect) in enumerate(rects):\n","#     # determine the facial landmarks for the face region, then\n","#     # convert the facial landmark (x, y)-coordinates to a NumPy\n","#     # array\n","#     shape = predictor(gray, rect)\n","#     shape = face_utils.shape_to_np(shape)\n","#     # convert dlib's rectangle to a OpenCV-style bounding box\n","#     # [i.e., (x, y, w, h)], then draw the face bounding box\n","#     (x, y, w, h) = face_utils.rect_to_bb(rect)\n","#     cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n","#     # show the face number\n","#     cv2.putText(image, \"Face {}\".format(i + 1), (x - 10, y - 10),\n","#         cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","#     # loop over the (x, y)-coordinates for the facial landmarks\n","#     # and draw them on the image\n","#     for (x, y) in shape:\n","#         cv2.circle(image, (x, y), 2, (0, 0, 255), -1)\n","# # show the output image with the face detections + facial landmarks\n","# cv2_imshow(image)\n","# cv2.waitKey(0)"],"execution_count":59,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":230},"id":"Csx-gjl-i1rx","executionInfo":{"status":"error","timestamp":1614266083716,"user_tz":480,"elapsed":457,"user":{"displayName":"Karthik Mittal","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgeZI506CDkDIyhAClP9oL5Mc8vkibyG3cGUA61jw=s64","userId":"10968410699437700240"}},"outputId":"9ccfe855-502b-479a-da41-6976a2167260"},"source":["for (i, rect) in enumerate(rects):\n","    # determine the facial landmarks for the face region, then\n","    # convert the landmark (x, y)-coordinates to a NumPy array\n","    shape = predictor(gray, rect)\n","    shape = face_utils.shape_to_np(shape)\n","    # loop over the face parts individually\n","    a_dict = face_utils.FACIAL_LANDMARKS_IDXS.items()\n","    tuple_list = list(a_dict)\n","    key_value = tuple_list[7]\n","    print(key_value)\n","    \n","    \n","    # show the particular face part\n","    name = key_value[0]\n","    i = key_value[1][0]\n","    j = key_value[1][1]\n","\n","    (x1, y1, w1, h1) = cv2.boundingRect(np.array([shape[i:j]]))\n","    print(i, j, name)\n","    roi = image[y1:y1 + h1 + 50, x1:x1 + w1]\n","    roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n","    # cv2_imshow(roi)\n","    cv2.imwrite('/content/gdrive/MyDrive/personality_datasets/headshot_jaw2.jpg', roi)\n","    # clone the original image so we can draw on it, then\n","    # display the name of the face part on the image\n","    # clone = image.copy()\n","    # cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n","    #     0.7, (0, 0, 255), 2)\n","    # # loop over the subset of facial landmarks, drawing the\n","    # # specific face part\n","    # for (x, y) in shape[i:j]:\n","    #     cv2.circle(clone, (x, y), 1, (0, 0, 255), -1)\n","    #     print((x,y))\n","    # # extract the ROI of the face region as a separate image\n","    \n","    # cv2_imshow(clone)\n","    # cv2.waitKey(0)\n","    # # visualize all facial landmarks with a transparent overlay\n","    # output = face_utils.visualize_facial_landmarks(image, shape)\n","    # cv2_imshow(output)\n","    # cv2.waitKey(0)"],"execution_count":11,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-4515ce61aea4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;31m# determine the facial landmarks for the face region, then\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# convert the landmark (x, y)-coordinates to a NumPy array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mface_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_to_np\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'dlib.rectangle' object is not iterable"]}]},{"cell_type":"code","metadata":{"id":"7FyHDBWWj4Xm"},"source":[""],"execution_count":null,"outputs":[]}]}